import csv
import os
import json
import subprocess
from datetime import datetime
from tqdm import tqdm
from sentence_transformers import SentenceTransformer, util

# –ü—É—Ç–∏
date_str = datetime.now().strftime("%Y-%m-%d")
enbek_file = f"./csv/enbek/parsed_data_enbek_{date_str}.csv"
hh_file = f"./csv/hh/parsed_data_hh_{date_str}.csv"
keywords_file = "keywords.json"

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è CSV
if not os.path.exists(enbek_file):
    subprocess.run(["python", "job-listings-enbek.py"], check=True)
if not os.path.exists(hh_file):
    subprocess.run(["python", "job-listings-hh.py"], check=True)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
with open(keywords_file, encoding='utf-8') as f:
    keyword_map = json.load(f)

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–µ–π –∏–∑ –∫–ª—é—á–µ–π
specialties = list(keyword_map.keys())
specialty_embeddings = model.encode(specialties, convert_to_tensor=True)

# –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ–π –∫–∞—Ä—Ç—ã: –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ ‚Üí —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å
keyword_to_spec = {}
for spec, keywords in keyword_map.items():
    for keyword in keywords:
        keyword_to_spec[keyword.lower()] = spec

# –°—Ç—Ä—É–∫—Ç—É—Ä–∞: specialty ‚Üí {count, vacancies}
spec_data = {spec: {"count": 0, "vacancies": []} for spec in specialties}
spec_data["–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ"] = {"count": 0, "vacancies": []}

# –§—É–Ω–∫—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
def classify(name):
    name_lower = name.lower()
    for keyword, spec in keyword_to_spec.items():
        if keyword in name_lower:
            return spec
    # –ú–æ–¥–µ–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    emb = model.encode(name, convert_to_tensor=True)
    similarities = util.pytorch_cos_sim(emb, specialty_embeddings)[0]
    best_idx = similarities.argmax().item()
    return specialties[best_idx]

# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞
def process_file(filename):
    with open(filename, encoding="utf-8") as f:
        reader = list(csv.DictReader(f))
        for row in tqdm(reader, desc=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {filename}"):
            name = row["name"]
            url = row.get("url", "")
            city = row.get("city", "–ù–µ —É–∫–∞–∑–∞–Ω–æ")
            count = int(row.get("count", 1))
            spec = classify(name)
            if spec not in spec_data:
                spec = "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ"
            spec_data[spec]["count"] += count
            spec_data[spec]["vacancies"].append({
                "name": name,
                "city": city,
                "url": url
            })

# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤
process_file(enbek_file)
process_file(hh_file)

# –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
spec_data = {k: v for k, v in sorted(spec_data.items(), key=lambda x: -x[1]["count"])}

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
os.makedirs("./json", exist_ok=True)
output_path = f"./json/grouped_by_specialty_{date_str}.json"
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(spec_data, f, ensure_ascii=False, indent=2)

print(f"‚úÖ JSON —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")

# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ index.json
index_path = "./json/index.json"

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º index.json
if os.path.exists(index_path):
    with open(index_path, "r", encoding="utf-8") as f:
        index_data = json.load(f)
else:
    index_data = {"all": []}

file_name = os.path.basename(output_path)
if file_name not in index_data["all"]:
    index_data["all"].append(file_name)

index_data["latest"] = file_name

with open(index_path, "w", encoding="utf-8") as f:
    json.dump(index_data, f, ensure_ascii=False, indent=2)

print(f"üìå index.json –æ–±–Ω–æ–≤–ª—ë–Ω: {index_path}")